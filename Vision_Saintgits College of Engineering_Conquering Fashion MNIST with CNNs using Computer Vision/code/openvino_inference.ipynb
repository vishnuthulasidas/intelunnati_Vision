{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ba681a2-4123-43d9-af59-8fe3536d499b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-13 21:25:25.834975: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-07-13 21:25:39.123383: E itex/core/kernels/xpu_kernel.cc:38] XPU-GPU kernel not supported.\n",
      "If you need help, create an issue at https://github.com/intel/intel-extension-for-tensorflow/issues\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1963e592-3ea7-452b-b927-eeff29242200",
   "metadata": {},
   "source": [
    "<h3>Loading the Fashion MNIST Test dataset from Tensorflow-Keras</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1774f1ec-b1ec-4e0a-bfa1-4f4b8bc3dde7",
   "metadata": {},
   "outputs": [],
   "source": [
    "(_, _), (test_images, test_labels) = keras.datasets.fashion_mnist.load_data()\n",
    "\n",
    "test_images = test_images.reshape((-1, 28, 28, 1))\n",
    "test_images = test_images.astype(\"float32\")/255"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe84615-0bb9-48b4-ba84-e46bcfb767cf",
   "metadata": {},
   "source": [
    "<h3>Optimising the model to IR format with OpenVino Model Optimiser</h3>\n",
    "\n",
    "* Output is saved in models directory as IR_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8faa81ec-9038-4ec5-bd62-e45e9bd4aba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-13 21:28:39.165368: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-07-13 21:28:43.790642: E itex/core/kernels/xpu_kernel.cc:38] XPU-GPU kernel not supported.\n",
      "If you need help, create an issue at https://github.com/intel/intel-extension-for-tensorflow/issues\n",
      "[ INFO ] The model was converted to IR v11, the latest model format that corresponds to the source DL framework input/output format. While IR v11 is backwards compatible with OpenVINO Inference Engine API v1.0, please use API v2.0 (as of 2022.1) to take advantage of the latest improvements in IR v11.\n",
      "Find more information about API v2.0 and IR v11 at https://docs.openvino.ai/2023.0/openvino_2_0_transition_guide.html\n",
      "[ INFO ] IR generated by new TensorFlow Frontend is compatible only with API v2.0. Please make sure to use API v2.0.\n",
      "Find more information about new TensorFlow Frontend at https://docs.openvino.ai/2023.0/openvino_docs_MO_DG_TensorFlow_Frontend.html\n",
      "[ SUCCESS ] Generated IR version 11 model.\n",
      "[ SUCCESS ] XML file: /home/u194052/intel_unnati_project/models/IR_model/saved_model.xml\n",
      "[ SUCCESS ] BIN file: /home/u194052/intel_unnati_project/models/IR_model/saved_model.bin\n"
     ]
    }
   ],
   "source": [
    "!mo --saved_model_dir ../models/itex_saved_model --output_dir ../models/IR_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb796f3-8482-4924-baa6-ba1ad6196efc",
   "metadata": {},
   "source": [
    "<h3>Importing the Openvino Runtime and loading the IR Model</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f9473eb1-8f4b-4276-ad91-29cdc6cfbe6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openvino.runtime as ov\n",
    "\n",
    "core = ov.Core()\n",
    "\n",
    "compiled_model = core.compile_model(\"../models/IR_model/saved_model.xml\", \"AUTO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9fc5928a-5564-43d2-849e-f77534292727",
   "metadata": {},
   "outputs": [],
   "source": [
    "infer_request = compiled_model.create_infer_request()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b113a7b2-2c19-4968-bce5-d2f3fb8203c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_array = np.array(test_images)\n",
    "\n",
    "image_array = image_array.astype(np.float32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a11881-4f01-4393-92be-9b6966584ae4",
   "metadata": {},
   "source": [
    "<h3>Input tensor is defined and set</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "432a82b0-d5b3-4e2c-90b4-ee218d69c0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor = ov.Tensor(array=image_array, shared_memory=True)\n",
    "\n",
    "infer_request.set_input_tensor(input_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bfd2d8f-1377-4634-b4c2-3c133b5c0cd0",
   "metadata": {},
   "source": [
    "<h3>Output is predicted using Openvino Optimised model with Openvino Runtime API and Inferrence time is noted</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7bfb48f5-c9f6-49bd-8899-3fbf25182506",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "output = infer_request.infer(input_tensor)\n",
    "end_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1902017c-bc5e-4607-97e2-6f695df70ba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenVino inference time : 1.6626226902008057\n"
     ]
    }
   ],
   "source": [
    "inference_time = end_time - start_time\n",
    "print(f\"OpenVino inference time : {inference_time}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36d5a37-b206-417a-9a0d-8aaabbe6d036",
   "metadata": {},
   "source": [
    "<h3>Output is predicted using normal model trained with Intel-tensorflow-extension and inferrence time is noted</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "08a9f174-e4d3-4994-a93a-721413e5ccfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model('../models/itex_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4e0240ba-f162-4dc3-97a4-0f27e3a9395d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1/313 [..............................] - ETA: 48s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-13 21:28:59.003423: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type CPU is enabled.\n",
      "2023-07-13 21:28:59.010291: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type CPU is enabled.\n",
      "2023-07-13 21:28:59.020625: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type CPU is enabled.\n",
      "2023-07-13 21:28:59.081413: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type CPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 3s 10ms/step\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "model.predict(test_images)\n",
    "end_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3391d830-446c-451d-b317-e0a4181e993f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.283219814300537\n"
     ]
    }
   ],
   "source": [
    "inference_time = end_time - start_time\n",
    "print(inference_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f580072-f878-4b89-9ad1-d195be4b0cdb",
   "metadata": {},
   "source": [
    "<h2>Inference time</h2>\n",
    "\n",
    "* with Openvino: 1.5 sec\n",
    "* without Openvino: 5.2 sec\n",
    "<h1>~70% reduction in Inferrence time is observed with OpenVino optimisation</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310c5604-1637-4545-a4db-e0e234766c67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (IntelÂ® oneAPI 2023.1)",
   "language": "python",
   "name": "c009-intel_distribution_of_python_3_oneapi-beta05-python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
